---
title: "Data Cuaca yang Dikumpulkan Otomatis dengan Github Action"
format: 
  gfm:
    html-math-method: webtex
    toc: false
    toc-depth: 4
    fig-dpi: 500
    fig-width: 8
    fig-height: 4.5
editor: visual
execute:
  echo: false
  warning: false
  error: false
---

```{r}
#| include: false

rm(list=ls())

library(dplyr)
library(tidyr)
library(ggplot2)
library(epoxy)

load("~/ARFWSNMRP/Clean Data/data all.rda")

kota = df_final %>% pull(kota) %>% unique() %>% sort()
```

Pada 2021 lalu, saya sempat menuliskan bagaimana cara mendapatkan data cuaca _real time_ dari [situs _openweathermap_](https://ikanx101.com/blog/open-cuaca/). Beberapa bulan lalu, saya juga menuliskan bagaimana cara membuat Github bisa melakukan [_web scrape_ secara otomatis](https://ikanx101.com/blog/gh-action/).

Nah, kali ini saya akan mengawinkan kedua teknik tersebut agar Github bisa _web scrape_ data cuaca _real time_ per dua jam sekali.

_Workflow_ github _action_-nya saya buat seperti ini:

```
on:
  #push:
    #branches: main
  schedule:
    - cron: "0 */2 * * *"  # run every 3 hrs
    
jobs:
  import-data:
    runs-on: ubuntu-latest
    steps:
      - name: Set up R
        uses: r-lib/actions/setup-r@v2

      - name: Install packages
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          packages: |
            any::dplyr 
            any::jsonlite

      - name: Check out repository
        uses: actions/checkout@v3

      - name: Import data
        run: Rscript -e 'source("real feel.R")'

      - name: Commit results
        run: |
          git config --local user.email "xxx@gmail.com"
          git config --local user.name "xx"
          git add .
          git commit -m 'Data updated' 
          git push 
```

Sementara skrip algoritma _web scrape_-nya saya simpan sebagai `real feel.R` (silakan lihat _postingan_ [saya sebelumnya](https://ikanx101.com/blog/open-cuaca/)).

Apa saja data yang saya dapatkan? Berikut adalah _variables_-nya:

```{r}
colnames(df_final)[-1]
```

Kenapa mengambil data cuaca dari situs [Open Weather Map](openweathermap.org)? Alasannya:

1.  **BMKG**:
    -   **API** dari situs **BMKG** tidak *reliable* dan sulit digunakan.
    -   Tidak ada data *real feel* di situs **BMKG**.
2.  [Open Weather Map](openweathermap.org):
    -   API dari [Open Weather Map](openweathermap.org) cukup *reliable* untuk diambil datanya **per dua jam**.
    -   Ada data *real feel*.
    -   Cakupan kota lumayan.
    -   Proses *scrape* bisa dilakukan otomatis tanpa supervisi sama sekali.

---

# DATA YANG DIAMBIL

```{r}
#| include: false

kota_sel = "Semarang"

bikin_grafik = function(kota_sel){
  df_final %>% 
  filter(kota == kota_sel) %>% 
  mutate(jam = lubridate::hour(waktu)) |>
  filter(jam >= 8 & jam <= 18) |>
  mutate(tanggal = lubridate::date(waktu)) |>
  group_by(tanggal) |>
  summarise(feel_like = mean(feel_like)) |>
  ungroup() |>
  ggplot(aes(x = tanggal,
             y = feel_like)) +
  geom_smooth(method = "loess",alpha = .3) +
  geom_line(color = "gray") +
  geom_point() +
  ylim(20,45) +
  theme_minimal() +
  labs(x = "Tanggal",
       y = "Feel Like (dalam Celcius)",
       color = "Kondisi Cuaca",
       title = paste0("Feel Like di ",kota_sel),
       subtitle = "Sumber: openweather.org") +
  theme(legend.position = "bottom")
  }

bikin_grafik_2 = function(kota_sel){
  df_final %>% 
  filter(kota == kota_sel) %>% 
  mutate(jam = lubridate::hour(waktu)) |>
  filter(jam >= 8 & jam <= 18) |>
  mutate(tanggal = lubridate::date(waktu)) |>
  group_by(tanggal) |>
  summarise(humidity = mean(humidity)) |>
  ungroup() |> 
  ggplot(aes(x = tanggal,
             y = humidity)) +
  geom_smooth(method = "loess",alpha = .3) +
  geom_line(color = "gray") +
  geom_point() +
  #ylim(20,45) +
  theme_minimal() +
  labs(x = "Tanggal",
       y = "Humidity",
       color = "Kondisi Cuaca",
       title = paste0("Humidity di ",kota_sel),
       subtitle = "Sumber: openweather.org") +
  theme(legend.position = "bottom")
}


bikin_sebaran = function(kota_sel){
  df_temp = 
  df_final %>% 
  filter(kota == kota_sel) |>
  mutate(jam = lubridate::hour(waktu)) |>
  filter(jam >= 8 & jam <= 18)

  mean_sel   = df_temp$feel_like %>% mean() %>% round(1)
  median_sel = df_temp$feel_like %>% median() %>% round(1)
  pesan      = paste0("Mean: ",mean_sel,"C\nMedian: ",
                      median_sel,"C")

  df_temp %>% 
    ggplot(aes(y = feel_like)) +
    geom_boxplot(color = "black",
                 fill  = "red",
                 alpha = .3) +
    theme_minimal() +
    labs(x = kota_sel,
         y = "Feel Like (dalam Celcius)",
         title = paste0("Boxplot Feel Like di ",kota_sel),
         subtitle = "Sumber: openweather.org") +
    theme(axis.text.x = element_blank()) +
    annotate("label",x = 0,y = median_sel,label = pesan)
}

```

Pada bagian ini, saya akan tunjukkan beberapa data cuaca di kota __Semarang, Jakarta, dan Surabaya__. Namun saya akan menggunakan data cuaca pada waktu jam kerja saja, yakni **pukul 08.00 - 18.00**.

## Sebaran Data

Berikut adalah sebaran data di kota-kota tersebut:

```{r}
#| fig-align: center

bikin_sebaran("Semarang")
bikin_sebaran("Jakarta")
bikin_sebaran("Surabaya")
```

## *Trend Feel Like*

Berikut adalah *trend feel like* dari kota-kota tersebut:

```{r}
#| fig-align: center
#| warning: false
#| message: false

bikin_grafik("Semarang")
bikin_grafik("Jakarta")
bikin_grafik("Surabaya")
```

## *Trend Humidity*

Berikut adalah *trend humidity* dari kota-kota tersebut:

```{r}
#| fig-align: center
#| warning: false
#| message: false

bikin_grafik_2("Semarang")
bikin_grafik_2("Jakarta")
bikin_grafik_2("Surabaya")
```

