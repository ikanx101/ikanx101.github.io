---
title: "Mengatasi Error Saat Melakukan Web Scraping"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/cloud/project/_posts/web scraping/post 4 error 404")
rm(list=ls())
library(rvest)
library(dplyr)
```

_Data science_ bukan hanya berkisah tentang _data carpentry_, _data analysis_, dan _machine learning_ tapi juga berkisah tentang _data acquisition_.

Salah satu contoh nyata _data acquisition_ yang sering dilakukan adalah _web scraping_.

Saya telah menuliskan beberapa tulisan terkait [_web scraping_](https://ikanx101.com/tags/#web-scrap). Namun saya belum pernah menuliskan terkait __resiko__ yang sering dihadapi saat melakukan _web scraping_.

Apa itu?

> ___Kemungkinan terjadinya ERROR.___

Kita bisa menemukan banyak tipe kode _error_ tapi yang paling sering saya temukan adalah __error 403__ dan __error 404__.

Sejatinya _errors_ tersebut sudah sewajarnya terjadi saat _web link_ yang kita tuju memang tidak ditemukan.

Tapi ada kondisi di mana _web link_-nya _exist_ tapi __R__ menampilkan pesan _error_ tersebut. Biasanya saya sering mengalami hal ini saat saya melakukan _loop web scraping_ pada ratusan hingga ribuan _web links_.

Mengapa hal ini bisa terjadi?

> ___Saya pribadi tidak mengetahui pasti alasan pastinya. Namun saya menduga___ `robots.txt` ___dari web tersebut memutuskan koneksi sebagai bentuk perlindungan.___ _cmiiw yah_.

## Solusi

Salah satu solusi yang biasa dilakukan para _web scraper_ di __R__ adalah dengan menuliskan tambahan _function_ `tryCatch()` pada saat membuat algoritma _scraper_-nya.

Hal ini menurut saya hanyalah solusi jangka pendek karena saya tidak ingin _web_ yang sebenarnya _exist_ namun _error_ saat dibaca menjadi di-_skip_.

> ___Saya ingin memaksa R untuk mencoba terus web tersebut beberapa kali sampai akhirnya skip saat benar-benar tidak bisa di-scrape___.


### Bagaimana caranya?
