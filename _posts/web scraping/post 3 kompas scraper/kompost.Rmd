---
title: "Media Monitoring Menggunakan R: Berita Pajak Mobil"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/cloud/project/_posts/web scraping/post 3 kompas scraper")
library(dplyr)
library(tidytext)
library(tidyr)
library(igraph)
library(ggraph)
library(ggplot2)

rm(list=ls())
load("kompas.rda")
```

Pada 2010 - 2012 lalu, saya sempat bekerja di salah satu _multinational market research agency_ di kawasan Sudirman. Sepengetahuan saya waktu itu, salah satu tim riset _business to consumer_ (__B2C__) biasa melakukan studi _media monitoring_.

> Apa yang mereka lakukan?

Mereka memantau berita-berita yang ada di beberapa portal dan obrolan-obrolan yang ada di forum digital terkait beberapa hal yang penting untuk diketahui oleh klien. Proses pemantauan sendiri dilakukan sangat manual saat itu.

## Kompas Money

Beberapa pekan yang lalu, saya mengobrol dengan salah seorang teman saya yang ternyata sedang berinisiasi untuk melakukan _media monitoring_ di kantornya. Tidak serumit apa yang kami lakukan dulu di _market research agency_. Teman saya hanya ingin melakukan _media monitoring_ secara spesifik dari situs _kompas.com_ bagian finansial ([Kompas Money](https://money.kompas.com/)).

Salah satu ide yang saya kemukakan adalah penggunaan __R__ untuk melakukan _web scraping_ secara berkala terhadap semua berita yang muncul di __Kompas Money__. Jadi teman saya cukup membuat satu algoritma sederhana untuk _web scraping_ semua berita yang ada di sana.

> Iya! Semua berita di Kompas Money!

Setelah itu, tinggal dilakukan analisa yang dibutuhkan dari hasil _scrape_-nya.

### Kompas Scraper _Function_

Di saat santai saya coba bantu membuatkan satu _function_ di __R__ yang berguna untuk mengambil beberapa informasi berikut di __Kompas Money__:

1. Siapa jurnalis penulis beritanya.
1. _Timestamp_ berita di-_publish_.
1. Judul berita.
1. Isi berita.

Berikut adalah _function_-nya:

```{r}
kompasin_donk = function(url){
  scrape = read_html(url) %>% html_nodes("p") %>% html_text()
  penulis = read_html(url) %>% html_nodes("#penulis a") %>% html_text()
  penulis = ifelse(length(penulis) == 0,NA,penulis)
  waktu = read_html(url) %>% html_nodes(".read__time") %>% html_text()
  judul = scrape[1]
  n = length(scrape) - 1
  isi = scrape[2:n]
  isi = paste(isi,collapse = " ")
  data = data.frame(jurnalis = penulis,
                    timestamp = waktu,
                    headline = judul,
                    berita = isi)
  return(data)
}
```

Ingat _yah_, _function_ di atas adalah untuk melakukan _web scraping_ dari satu alamat situs dari _Kompas Money_. Jika ingin melakukan _web scraping_ untuk semua berita yang ada, kita cukup _scrape_ terlebih dahulu semua _links_ berita yang ada.

> Caranya Bagaimana?

Gunakan saja cara berikut ini:

```{r}
#global = c(paste0("https://money.kompas.com/whatsnew/",1:10))
#links = c()
#for(i in 1:10){
#  temp = read_html(global[i]) %>% html_nodes("a") %>% html_attr("href")
#  links = c(links,temp)
#}
#links = links[grepl("money",links)]
#links = links[grepl("read",links)]
#links_1 = unique(links)
#links_2 = paste0(links_1,"?page=2")
#links = c(links_1,links_2)
```

### Data Hasil _Web Scraping_

Per `2 Maret 2021` pukul `8.40 WIB`, tercatat ada `154` berita yang berhasil saya ambil dari _Kompas Money_. Berikut adalah contoh sampel data yang saya dapatkan:

```{r,echo=FALSE,warning=FALSE,message=FALSE}
clean_data %>% head(5) %>% knitr::kable()
```

Lantas analisa apa yang bisa dilakukan? 

Sebenarnya banyak yang bisa dilakukan. Contohnya, teman saya ingin melakukan _sentimen analysis_ terkait beberapa topik berita yang ada. Kita juga bisa melihat apakah ada perbedaan gaya penulisan dari masing-masing jurnalis di _Kompas.com_. 

Kali ini saya ingin melakukan sesuatu yang sederhana. Apa itu?

### _Preparation_ dan _Cleaning_

Sebelum jauh melakukan analisa, saya akan bersihkan terlebih dahulu data di atas sehingga menjadi berikut ini:

```{r,echo=FALSE,warning=FALSE,message=FALSE}
data_final = 
  clean_data %>% 
  mutate(judul = gsub("ppnbm","ppnbm",headline,ignore.case = T),
         judul = janitor::make_clean_names(judul),
         judul = gsub("\\_"," ",judul)) %>% 
  mutate(timestamp = gsub("Kompas.com - ","",timestamp),
         timestamp = gsub(" WIB","",timestamp)) %>% 
  separate(timestamp,
           into = c("tgl","jam"),
           sep = "\\,") %>% 
  mutate(tgl = trimws(tgl),
         jam = trimws(jam)) %>% 
  mutate(tgl = as.Date(tgl,"%d/%m/%Y")) %>% 
  select(-jam) %>% 
  mutate(berita = gsub("Tulis komentar dengan menyertakan tagar #JernihBerkomentar dan #MelihatHarapan di kolom komentar artikel Kompas.com. Menangkan E-Voucher senilai Jutaan Rupiah dan 1 unit Smartphone. ","",berita)) %>%
  rowwise() %>% 
  mutate(berita = gsub(headline,"",berita)) %>% 
  ungroup() %>% 
  select(-headline) %>% 
  mutate(berita = gsub("JAKARTA, KOMPAS.com - ","",berita)) %>% 
  relocate(judul,.before = berita) %>% 
  mutate(berita = trimws(berita))

head(data_final,5)
```


### Berita Terkait Pajak Mobil di Kompas Money

Misalkan saya ingin mengetahui semua berita terkait sebuah topik `pajak mobil` yang sedang hangat diperbincangkan.

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center'}
mobil = 
  data_final %>% 
  filter(grepl("mobil",berita,ignore.case = T)) %>% 
  filter(grepl("pajak",berita,ignore.case = T)) 

mobil %>% 
  group_by(tgl) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  ggplot(aes(x = factor(tgl),
             y = n)) +
  geom_line(group = 1,color = "steelblue") +
  geom_label(aes(label = n),size = 3) +
  labs(title = "Banyaknya Berita Terkait `Pajak Mobil`",
       subtitle = "sumber: Kompas Money",
       caption = "Scraped and Visualized\nusing R\nikanx101.com",
       x = "Tanggal",
       y = "Banyaknya berita") +
  theme_minimal() +
  theme(axis.text.y = element_blank())
```

Berita terbanyak muncul pada Senin, `1 Maret 2021` kemarin. Apa saja judul beritanya?

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center'}
wc = 
  mobil %>% 
  select(judul) %>% 
  unnest_tokens(words,judul,"words") %>% 
  count(words,sort = T) %>% 
  filter(!words %in% c("dengan","di","dan","ini","dari","yang","akan",
                       "untuk","dapat","rp"))

library(wordcloud)
wordcloud(words = wc$words,freq = wc$n, min.freq = 2)
```

Dari _wordcloud_ di atas kira-kira sudah ketebak _lah_ ya?

Sekarang saya akan melihat _bigrams_ yang muncul dari berita-berita tersebut.

```{r,echo=FALSE,warning=FALSE,message=FALSE}
mobil = 
  mobil %>% 
  mutate(berita = gsub("ppnbm","ppnbm",berita,ignore.case = T),
         berita = janitor::make_clean_names(berita),
         berita = gsub("\\_"," ",berita))

stop = readLines("https://raw.githubusercontent.com/ikanx101/ID-Stopwords/master/id.stopwords.02.01.2016.txt")


mobil %>% 
  unnest_tokens(words,berita,token = "words") %>% 
  filter(!words %in% stop) %>% 
  group_by(judul) %>% 
  summarise(berita = paste(words,collapse = " ")) %>% 
  ungroup() %>% 
  unnest_tokens(bigrams,berita,token = "ngrams",n=2) %>% 
  count(bigrams,sort = T) %>% 
  filter(n > 5) %>% 
  separate(bigrams,into = c("from","to"),sep = " ") %>%
  graph_from_data_frame() %>% ggraph(layout = 'fr') +
  geom_edge_bend(aes(edge_alpha=n), show.legend = F, color='darkred') +
  geom_node_point(size=1,color='steelblue') +
  geom_node_text(aes(label=name),alpha=0.4,size=3,repel = T) + 
  theme_void()

```

---

# _Summary_

Dengan __R__ saya bisa melakukan _media monitoring_ dengan relatif mudah. Mulai dari proses _web scraping_, analisa, hingga _reporting_ dapat dilakukan di dalam __R__.

---

`if you find this article helpful, support this blog by clicking on the ads shown.`