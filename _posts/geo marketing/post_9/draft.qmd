---
title: "Untitled"
format:
  gfm:
    html-math-method: webtex
fig-dpi: 500
fig-width: 8
fig-height: 4.5
editor: visual
execute:
  warning: false
  echo: false
error: false
---

```{r}
#| include: false

setwd("~/ikanx101.github.io/_posts/geo marketing/post_9")

rm(list=ls())
gc()

library(dplyr)
library(tidyr)
library(ggplot2)
library(ggpubr)
library(ggmap)
library(janitor)
```

Beberapa minggu belakangan, beberapa departemen meminta kepada saya untuk melakukan *web scraping* beberapa kategori *point of interest* dari **Google Maps** dan **Google Places**. Tujuan mereka sederhana, yakni:

> Menentukan area atau POI mana yang harus digarap pada tahun ini. Syukur-syukur penggarapan ini akan meningkatkan omset perusahaan.

Setelah saya *scrape* dan berikan data kepada mereka, artinya **sudah menjadi kewajiban mereka** untuk menentukan area dan POI mana yang harus digarap. Lantas bagaimana caranya? *Jujurly*, saya kurang mengetahui hal tersebut tapi saya yakin mereka sudah punya pakem tersendiri.

------------------------------------------------------------------------

Sampai saat ini, saya sudah menuliskan [12 *posts* di blog saya ini terkait *geo marketing*](https://ikanx101.com/tags/#geomarketing). Jika saya hendak membantu rekan-rekan di kantor untuk **menentukan area dan POI yang hendak digarap**, apakah bisa digunakan cara seperti yang pernah saya tulis dari salah satu dari 12 *posts* tersebut? **Jawabannya *TENTU BISA!***.

> Saya pernah menuliskan beberapa analisa di *geo marketing* yang bisa jadi basis pengambilan keputusan tersebut.

*Nah*, pada *post* kali ini saya akan melakukan beberapa alternatif analisa lain yang belum pernah saya lakukan sebelumnya pada topik *geo marketing*. Beberapa analisa ini sebenarnya sudah pernah saya lakukan di topik lainnya tapi ternyata *works well* juga di *geo marketing*.

__Rekan-rekan bisa melihat setiap analisa yang kelak saya tuliskan di bawah sebagai analisa-analisa yang terpisah__ sehingga tidak ada kewajiban untuk disangkutpautkan sama sekali antar hasilnya.

Sebagai contoh, saya akan melakukan analisa untuk POI berupa **tempat makan kategori tertentu** di Kota Surabaya, tepatnya di Kecamatan Gubeng dan Genteng.


```{r}
load("analisa_1.rda")

key = "AIzaSyBlZpsnqIhuJDHZ-nGo1VrU3PmKDW18-ls"

register_google(key = key)

lisbon_satellite <- get_map("Kota Surabaya",
                            source = "google",
                            api_key = api_secret,zoom=13)

basis =
  df_sel %>%
  select(lat,long) %>%
  distinct()

ggmap(lisbon_satellite) +
  geom_point(data = basis,aes(x = long,y = lat),
             size = .9,
             color = "steelblue") +
  theme_void() +
  coord_equal()

```

Apa saja analisanya? *Cekidot!*

## Alternatif Analisa I: Kuadran *Rating* vs *User*

Analisa pertama yang hendak saya lakukan sebenarnya sangat sederhana, yakni cukup dengan membuat *scatter plot* antara *rating* dan berapa banyak *user* yang memberikan *rating*. Tujuannya adalah memetakan tempat makan mana yang termasuk ke dalam kuadran-kuadran sebagai berikut:

|                   | Review sedikit         | Review Banyak        |
|-------------------|------------------------|----------------------|
| **Rating tinggi** | *The Hidden gems*      | *The Populars*       |
| **Rating rendah** | *The Under performers* | *The Public critics* |

Dari peta ini, tentu strategi yang dijalankan per masing-masing kuadran akan berbeda. Mari kita lihat plot pertama sebagai berikut ini:

```{r}
p1
```

Kita bisa melihat bahwa kuadran yang dihasilkan secara visual sangat *mepet* dan tidak tersebar secara proporsional. Hal ini terjadi karena *range* dari data banyaknya *user* yang memberikan *rating* sangat jauh besar dan *skewed* ke salah satu sisi. Oleh karena itu, saya bisa melakukan transformasi dengan menggunakan fungsi logaritmik. Berikut adalah grafik kedua:

```{r}
p2
```

Setelah ditransformasi, sumbu *x* sudah relatif menyebar rata **tapi** data pada *rating* yang masih *skewed*. Jika dibuat kuadran, maka bentuknya seperti ini:

```{r}
p3
```

```{r}
df_sel %>% 
  tabyl(kuadran) %>% 
  adorn_pct_formatting() %>% 
  adorn_totals() %>% 
  knitr::kable()
```

Dari analisa ini, tim terkait bisa memilih kuadran tertentu sebagai target POI yang hendak diajak kerja sama.

**TAPI menurut keyakinan saya**, untuk data dengan rentang yang terlalu mepet atau terlalu lebar dan tergambarkan sebagai *skewed distribution*, alternatif analisa ini bisa menimbulkan bias dan kurang tepat untuk digunakan.

## Alternatif Analisa II: *Spatial Clustering \>\< K-Means Clustering*

Pada analisa ini, saya akan melakukan [dua teknik *clustering* yang berbeda](https://ikanx101.com/blog/clustering-R/), yakni:

1.  DBscan untuk mengelompokkan tempat-tempat makan yang berdekatan di suatu area berdasarkan data *longlat*-nya.
2.  K-Means untuk mengelompokkan tempat-tempat makan berdasarkan *rating* dan berapa banyak *user* yang memberikan rating.

Tujuannya adalah menjawab pertanyaan berikut ini:

1.  Apakah tempat-tempat makan dengan *rating* tinggi cenderung berkumpul di area elit tertentu?
2.  Apakah ada pusat kuliner baru yang terbentuk secara organik?

Berikut adalah *summary* hasilnya:

![](dashboard_clustering.png){width="550"}

```{r}
rm(list=ls())
load("analisa_2.rda")
```

1. Ada 13 _clusters_ spasial plus 1 _noise cluster_ spasial.
1. Ada empat _clusters_ berdasarkan _rating_ dan berapa banyak _user_ yang memberikan _rating_. _Cluster_ __*The Viral Venues*__ dan __*The Exclusive*__ akan menjadi poin penting untuk menentukan area spasial kelak.

```{r}
kmeans_summary <- df_sel %>%
  group_by(kmeans_cluster_label) %>%
  summarise(
    n_restoran = n(),
    avg_rating = mean(rating, na.rm = TRUE),
    avg_user_rating = mean(user_rating, na.rm = TRUE),
    median_rating = median(rating, na.rm = TRUE),
    median_user_rating = median(user_rating, na.rm = TRUE),
    prop_high_rating = sum(rating >= 4.5) / n(),
    prop_popular = sum(user_rating >= 100) / n()
  )

kmeans_summary %>% knitr::kable()
```

Sekarang kita akan menjawab kedua pertanyaan yang ada dari grafik dan perhitungan yang sudah saya lakukan.

**Apakah restoran dengan _rating_ tinggi cenderung berkumpul di area elit tertentu?** 

Perhatikan grafik berikut ini:

![](plot_5.png){width="550"}

Plot ini adalah gabungan dari analisa _clustering spatial_ dengan _clustering_ berdasarkan _rating_ dan _user_. Jika tempat-tempat makan yang termasuk ke dalam _the Viral Venues_ dan _the Exclusive_ terkonsentrasi di area spasial tertentu, itu mengindikasikan suatu area termasuk ke dalam elit. Untuk menentukannya, saya akan membuat skor dengan membobotkan proporsi jenis tempat makan yang ada. 

Bobot tertinggi untuk tempat makan _the Viral Venues_, kemudian _the Exclusive_, lalu _the Trusted_. Sebagai contoh, bobot yang saya gunakan __tergantung dari keyakinan saya *ya*__.

```{r}
df_skor = 
  df_sel %>% 
  tabyl(dbscan_cluster,kmeans_cluster_label) %>% 
  adorn_percentages("row") %>% 
  mutate(skor = 2 * `The Viral Venues` + `The Exclusive` + .5 * `The Trusted`) %>% 
  arrange(desc(skor)) %>% 
  select(dbscan_cluster,skor) %>% 
  mutate(skor = skor * 100,
         skor = round(skor,2)) 

df_sel %>% 
  tabyl(dbscan_cluster,kmeans_cluster_label) %>% 
  adorn_percentages("row") %>%
  adorn_totals("col") %>% 
  adorn_pct_formatting() %>% 
  merge(df_skor) %>% 
  arrange(desc(skor)) %>% knitr::kable()
```

Saya dapatkan _cluster spasial_ ke 5 dan 9 memiliki skor tertinggi, yakni mereka yang berada pada:

```{r}
key = "AIzaSyBlZpsnqIhuJDHZ-nGo1VrU3PmKDW18-ls"

register_google(key = key)

lisbon_satellite <- get_map("Tunjungan Plaza, Kota Surabaya",
                            source = "google",
                            api_key = api_secret,zoom=15)

basis =
  df_sel %>%
  filter(dbscan_cluster %in% c(9,5)) %>% 
  select(lat,long,dbscan_cluster) %>%
  distinct()

ggmap(lisbon_satellite) +
  geom_point(data = basis,aes(x = long,y = lat,color = dbscan_cluster),
             size = 1.2) +
  scale_color_manual(values = c("blue","green")) +
  theme_void() +
  coord_equal() +
  labs(color = "Cluster Spasial ke-")
```

**Apakah ada pusat kuliner baru yang terbentuk secara organik?** 

Saya akan membuat definisi __tempat makan baru__ berdasarkan banyak _user_ yang memberikan _rating_. Jika "sedikit", maka saya asumsikan tempat makan tersebut merupakan tempat makan baru. Berikut ini adalah _cluster_ spasial yang memiliki tempat-tempat makan baru terbanyak di antara _clusters_ yang lain:

```{r}
cluster_baru = 
  df_sel %>% 
  mutate(baru = ifelse(user_rating < 30,T,F)) %>% 
  group_by(dbscan_cluster) %>% 
  summarise(new = sum(baru) / length(baru)) %>% 
  ungroup() %>% 
  arrange(desc(new)) %>% 
  head(3) %>% pull(dbscan_cluster)
  

lisbon_satellite <- get_map("Universitas Airlangga Kampus B, Kota Surabaya",
                            source = "google",
                            api_key = api_secret,zoom=14)

basis =
  df_sel %>%
  filter(dbscan_cluster %in% cluster_baru) %>% 
  select(lat,long,dbscan_cluster) %>%
  distinct()

ggmap(lisbon_satellite) +
  geom_point(data = basis,aes(x = long,y = lat,color = dbscan_cluster),
             size = 1.2) +
  scale_color_manual(values = c("blue","green","purple")) +
  theme_void() +
  coord_equal() +
  labs(color = "Cluster Spasial ke-")

```

Dari analisa alternatif ini, saya bisa mendapatkan beberapa area yang bisa digarap dengan definisi yang sudah saya tentukan. Tentunya definisi ini bisa disesuaikan dengan kebutuhan yang diperlukan (jika mau).

## Alternatif Analisa III: Koreksi _Bayesian Rating_

__Menurut keyakinan saya__, jika ada dua tempat makan yang:

1. Punya _rating_ 5 dari penilaian 50 orang _user_.
1. Punya _rating_ 5 dari penilaian 1.000 orang _user_.

__punya derajat (atau *rating*) yang berbeda__. Lebih banyak _user_ yang memberikan _rating_ membuat derajat kepercayaan atas penilaian lebih tinggi. Lantas agar _rating_ yang digunakan _fair enough_, apa yang harus saya lakukan? Saya akan melakukan perhitungan _Bayesian Rating Correction_ untuk mendapatkan estimasi _rating_ yang lebih _fair_ memanfaatkan informasi _prior_ berupa rata-rata _rating_ tempat makan populasi data saya.

> Konsep _Bayesian Rating_ adalah menggabungkan informasi dari _rating_ individu dengan _prior_ untuk mendapatkan _rating_ yang lebih akurat. Saya pernah menuliskan [beberapa topik terkait analisa Bayesian](https://ikanx101.com/tags/#bayesian) di _blog_ saya.

Untuk melakukan koreksi ini, saya hanya akan memilih tempat makan yang memiliki _rating_ dengan banyaknya _user_ lebih dari 30 orang. Kenapa? Karena jika terlalu kecil, koreksi yang dihasilkan punya _error_ lebih tinggi.


![](bayesian_rating_analysis.png){width="550"}

Dari 242 tempat makan yang saya proses, saya mendapatkan _mean rating original_ sebesar __4.479959__. Setelah dikoreksi, _mean rating Bayesian_-nya menjadi __4.480827__. Terlihat tidak ada perubahan yang signifikan dari nilai _mean_-nya namun sebaran data hasil koreksi menjadi lebih _compact_ di mana standar deviasinya berubah dari _original_ __0.2469339__ menjadi _Bayesian_ __0.12283__.

Ada satu hal yang perlu saya sampaikan juga: __tempat makan dengan *user* yang lebih banyak memberikan hasil *confidence* pengoreksian yang lebih tinggi juga__. Maka dari itu, saya coba kategorikan tiap tempat makan menjadi kelompok:

1. _Confidence_ koreksi __sangat rendah__, tempat makan dengan _user_ 30 - 50 orang.
1. _Confidence_ koreksi __rendah__, tempat makan dengan _user_ 50 - 100 orang.
1. _Confidence_ koreksi __sedang__, tempat makan dengan _user_ 100 - 150 orang.
1. _Confidence_ koreksi __tinggi__, tempat makan dengan _user_ 150 - 200 orang.
1. _Confidence_ koreksi __sangat tinggi__, tempat makan dengan _user_ di atas 200 orang.

Grafik di atas menampilkan perubahan _rating_ signifikan (baik meningkat atau turun) yang dialami oleh beberapa tempat makan. Sedangkan tabel berikut ini adalah __top 10__ tempat-tempat makan yang memiliki _rating_ tertinggi hasil koreksi:

```{r}
rm(list=ls())

df = read.csv("restaurant_ratings_bayesian_corrected.csv") %>% janitor::clean_names()

df %>% 
  arrange(desc(bayesian_rating)) %>% 
  head(10) %>% 
  select(nama,original_rating,bayesian_rating,user_count,confidence_category) %>% 
  knitr::kable()
```

## Alternatif Analisa IV: Analisis Hubungan Jarak dan Kualitas (_Spatial Correlation_)

Tujuan dari analisa ini adalah:

1. Apakah tempat-tempat makan yang letaknya __berdekatan__ cenderung memiliki _rating_ yang mirip?
1. Apakah kualitas tempat makan menular ke sebelahnya?

Untuk analisa ini, saya menggunakan [perhitungan jarak _real_ dari _Openstreetmaps_ pada mode __berjalan kaki__](https://ikanx101.com/blog/osrm-R/). Kemudian dua tempat makan disebut __berdekatan__ jika jaraknya maksimum 50 meter berjalan kaki. Perlu saya akui bahwa proses membentuk matriks jarak antar tempat makan menggunakan _Openstreetmaps_ membutuhkan waktu yang sangat amat lama.

Dari hampir enam ratus data tempat makan yang ada, saya mendapatkan xx pasang tempat makan yang masuk ke dalam definisi __berdekatan__. Jika saya hitung korelasi rating antar kedua tempat makan yang berdekatan, saya dapatkan:







Analisis: Hitung jarak antar restoran (Euclidean atau Haversine). Apakah ada pola di mana di satu jalan tertentu semua restorannya punya rating tinggi? Atau justru di area pusat wisata, rating cenderung lebih rendah karena "yang penting laku"?

Angle Blog: "Hukum Tetangga: "

5. Analisis "Bayesian Surprise" atau Anomali

Mencari restoran yang melawan arus di wilayahnya.

Analisis: Misalnya, di area yang rata-rata ratingnya 3.5, tiba-tiba ada satu restoran dengan rating 4.7.

Insight: Mengapa restoran ini bisa sangat unggul dibanding kompetitor di sekitarnya? (Bisa dikaitkan dengan jenis masakan atau harga jika ada datanya).

Angle Blog: "Melawan Arus: Menemukan Restoran Anomali di Tengah Rata-rata."


