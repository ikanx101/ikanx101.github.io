---
title: "Merangkum Tulisan Menggunakan Algoritma Extraction Based Summarization di R"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/ikanx101/_posts/kurasi/versi 1")
library(dplyr)
library(nomnoml)
library(EBImage)
```

> Suatu masih SD, SMP, hingga SMA dulu. Pernah beberapa kali guru menugaskan saya dan teman-teman sekelas untuk merangkum materi dari buku.

Untuk beberapa guru, malahan kami disuruh untuk mencari sumber selain buku yang digunakan di sekolah. 

---

## Bagaimana cara kita merangkum?

Setidaknya ada dua cara merangkum, yakni:

1. __Merangkum dengan kata-kata sendiri__. Biasanya kita harus membaca semua tulisan terlebih dahulu, lalu menuliskan apa yang penting sesuai dengan pemahaman kita. 
1. __Merangkum dengan menuliskan kata-kata sesuai dengan yang ada di buku__. Biasanya saat membaca semua tulisan, kita menandai kata atau kalimat mana yang penting. Lalu kita tuliskan ulang kalimat-kalimat tersebut apa adanya. 

Pertanyaannya:

> Bisakah kita membuat algoritma yang bisa merangkum sebuah tulisan?

Saya jawab _yah_: 

> __Jawabannya bisa!__

Sebelum saya membuat algoritmanya menggunakan __R__, mari saya perkenalkan dulu dua cara tersebut.

### Merangkum dengan kata-kata sendiri

Dalam dunia _data science_, merangkum dengan kata-kata sendiri disebut dengan _abstraction based summarization_.

Contoh sederhananya adalah sebagai berikut. Misalkan saya punya kalimat:

> Manchester United baru saja kalah dari RB Leipzig dengan skor 3-2 di laga penyisihan Liga Champion Eropa/

Maka hasil _abstraction based summarization_-nya kira-kira sebagai berikut:

> Manchester United dikalahkan RB Leipzig.

_Summary_ yang dihasilkan adalah kalimat baru hasil _rephrase_ dari konteks tulisan. Untuk bisa melakukan hal ini, kita perlu membuat model _deep learning_ yang rumit dan melatih model dengan berbagai macam prinsip-prinsip __NLP__ atau _language modelling_.

Analoginya itu seperti pulpen.

```{r out.width="50%",echo=FALSE}
pulpen = readImage("images.jpeg")
plot(pulpen)
```

Kita bisa menuliskan apapun yang kita mau untuk merangkum tulisan tersebut.

### Merangkum dengan menuliskan kata-kata yang sesuai di buku

Dalam dunia _data science_, merangkum dengan cara ini disebut dengan _extraction based summarization_.

Cara kerjanya relatif lebih mudah dibandingkan dengan cara pertama. Yakni dengan menentukan:

1. Kalimat yang `penting`.
1. Kalimat yang `sama`.

Analoginya seperti stabilo.

```{r out.width="50%",echo=FALSE}
stabilo = readImage("images (1).jpeg")
plot(stabilo)
```

Kita hanya memberikan _highlight_ terhadap kata atau kalimat yang menurut kita penting dari tulisan tersebut.

Saya akan membuat algoritma berdasarkan cara kedua ini.

---

# _Extraction Based Summarization_

## _Framework_

Misalkan saya memiliki suatu tulisan atau artikel atau buku yang panjang dan saya diharuskan untuk merangkumnya. Cara yang akan saya lakukan adalah menentukan kalimat mana saja yang penting. Setelah itu, dari kalimat-kalimat tersebut, saya akan cek bagaimana _similarity_ dari kata-kata tersebut.

Logikanya, jika terdapat dua atau lebih kalimat yang `sama`, saya hanya perlu menuliskan satu kalimat saja. Tidak perlu semuanya.

Berdasarkan _framework_ sederhana itu, saya akan membuat algoritmanya sebagai berikut:

```{r,echo=FALSE,fig.retina=10}
nomnoml("#direction: down,
        [Raw Data|Raw text] -> [Pre-Processing]
        [Pre-Processing] -> [Importance Screening]
        [Importance Screening] -> [Similarity Checking]
        [Similarity Checking] -> [Summarizing]
        
        [Pre-Processing|
          [Extract] -> [Paragraph]
          [Paragraph] -> [Sentences]
          [Sentences] -> [Pre-Processing]
          [Pre-Processing] -> [Lowercase]
          [Pre-Processing] -> [Remove]
          [Pre-Processing] -> [Stem]
          [Remove] -> [Stopwords]
          [Remove] -> [Punctuation|Kecuali titik]
          [Remove] -> [Numbers]
          [Stem] -> [Into root words]
          ]
        
        [Importance Screening|
          [Word counting] -> [Important words]
          [Important words] -> [Scoring|Relative score]
          [Scoring] -> [Sentences scoring]
          [Sentences scoring] -> [Important sentences]
          ]
        
        [Similarity Checking|
          [Measuring sentences similarity] -> [Cosine similarity]
          [Cosine similarity] -> [Distance matrix|Sentences as nodes\nSimilarity as edges]
          [Distance matrix] -> [Network Analysis]
          [Network Analysis] -> [Centrality|High Degree Nodes]
          ]
          
        [Summarizing|
          [Which sentences] -> [are important?]
          [Which sentences] -> [are centroids?]
          ]
        ")
```


Dari kesederhanaan _framework_ di atas, ada beberapa titik kritis yang perlu diperhatikan, yakni:

### _Pre-processing_

Dari tulisan yang ada, kita perlu mengetahui kalimat mana masuk ke dalam paragraf mana. Untuk itu, tanda `titik` menjadi krusial sebagai pembeda kalimat. Namun pada kenyataannya, `titik` tidak hanya digunakan sebagai penanda antar kalimat tapi bisa digunakan sebagai pengganti `koma` dalam _metric_ atau _currency_. 

Oleh karena itu, perlu ada algoritma yang bisa melakukan _tricky parts_ semacam ini memanfaatkan _regex_ (_reguler expression_) dari pola-pola karakter yang ada.

### _Importance_

Ini adalah proses yang termudah dari sekian langkah yang ada. Prinsipnya adalah _word counting_. Secara logika, kata-kata yang sering muncul merupakan kata yang penting dalam tulisan tersebut.

Agar tidak terjadi bias, maka _stopwords_ dan _numbers_ harus sudah dihapus di tahap _pre-processing_.

Langkah yang penting di sini adalah mengembalikan _importance score_ per kata kembali ke kalimat awalnya. Lalu menghitung _importance score_ dari masing-masing kalimat. 

Agar tidak terjadi bias, _importance score_ suatu kalimat dihitung dengan cara: $\frac{\Sigma score_{words}}{n_{words}}$.

### _Similarity_

_Similarity_ dari dua kalimat dihitung dengan _cosine similarity_.

_Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space._

Kita perlu menghitung angka _cosine similarity_ dari semua kombinasi pasangan kalimat yang ada. Oleh karena itu _output_ dari perhitungan ini berupa _matriks_.

> Sekarang sudah mengerti kenapa aljabar menjadi penting kan? _hehe_

Misalkan saya punya 51 buah kalimat. Maka akan terbentuk matriks `51 x 51` dengan isinya berupa _cosine similarity_. Matriks ini kita ubah menjadi matriks jarak.

Setelah itu, saya akan menentukan kata mana yang menjadi _centroid_ dengan menghitung _degree_ menggunakan prinsip [_network analysis_](https://ikanx101.com/blog/sna-feedback/).

Sebenarnya ada beberapa cara menentukan _centroids_ (pusat dari _network_ graf). Saya sudah mencoba menggunakan _degree_, _betweenness_, dan _eigen value_. Tapi _degree_ menghasilkan _summary_ yang lebih baik.

Saya juga sudah mencoba alternatif lain seperti algoritma _Travelling Salesperson Problem_ untuk _clustering_ _nodes_ berdasarkan jaraknya. Tapi cara _network analysis_ masih memberikan hasil _summary_ yang lebih baik.

### _Summarizing_

Langkah terakhir ini adalah menentukan _threshold_ untuk _importance score_ dan _degree_. Pada nilai berapa saja algoritma memilih kalimat yang diinginkan.
  
---

# _Case Study_

## _Toxic Seniority_

Sebagai contoh, saya akan mencoba melakukan _extraction based summarization_ dari tulisan di blog CEO [Nutrifood](https://www.nutrifood.co.id/) (Pak [Wumard](https://www.instagram.com/wumard/?hl=en)) berjudul: __Toxic Seniority__. 

> Bagi rekan-rekan yang menjadi _leader_ di organisasi apapun, saya menyarankan kalian membaca tulisan beliau _yah_.

Tulisan lengkap beliau bisa dibaca di [_link_ berikut ini](https://wumard.wordpress.com/2020/09/20/toxic-seniority).

Berikut adalah data yang saya _scrape_:

```{r,echo=FALSE}
rm(list=ls())
load("bahan_blog.rda")
data_awal_blog
```

Jika saya pecah ke bentuk kalimat, hasilnya sebagai berikut:

```{r,echo=FALSE}
print("7 Kalimat Teratas")
head(data_dua_blog,7)
```

Jika saya cari _importance score per words_ lalu saya kembalikan kembali ke data di atas, didapatkan:

```{r,echo=FALSE}
print("7 Kalimat Teratas")
head(data_tiga_blog,7)
```

Lalu bentuk _network analysis_-nya sebagai berikut:

```{r,echo=FALSE,fig.retina=10,warning=FALSE,message=FALSE}
library(igraph)
library(ggraph)

matrix_blog %>% 
  graph_from_incidence_matrix() %>% 
  ggraph(layout = "fr") +
  geom_edge_arc(alpha = .3,
                color = "darkgreen") +
  geom_node_point(color = "darkred",
                  size = 5) +
  labs(title = "Network Analysis dari Semua Kalimat yang Ada",
       subtitle = "Toxic Seniority",
       caption = "Scraped, Computed, and Visualized\nusing R\nikanx101.com")
```

Menggunakan _threshold_ berupa _mean_ dari _importance_ dan _degree_, maka hasil rangkuman dari tulisan ini adalah:

```{r,echo=FALSE}
kesimpulan = strsplit(kesimpulan,split = "\\.")
kesimpulan[[1]]
```

---

# _Summary_

Algoritma ini sudah dicoba untuk berbagai macam tipe artikel. Saya pribadi cukup puas dengan kinerja algoritma ini walaupun ada beberapa celah yang bisa terus dikembangkan.

Salah satu kelemahan algoritma ini adalah saat menemui artikel yang dipenuhi dengan angka.