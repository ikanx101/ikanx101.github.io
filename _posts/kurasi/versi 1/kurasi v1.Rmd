---
title: "Merangkum Tulisan Menggunakan Algoritma"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(nomnoml)
```

> Suatu masih SD, SMP, hingga SMA dulu. Pernah beberapa kali guru menugaskan saya dan teman-teman sekelas untuk merangkum materi dari buku.

Untuk beberapa guru, malahan kami disuruh untuk mencari sumber selain buku yang digunakan di sekolah. 

---

## Bagaimana cara kita merangkum?

Setidaknya ada dua cara merangkum, yakni:

1. Merangkum dengan kata-kata sendiri.
1. Merangkum dengan menuliskan kata-kata sesuai dengan yang ada di buku.

Pertanyaannya:

> Bisakah kita membuat algoritma yang bisa merangkum sebuah tulisan?

Saya jawab _yah_: 

> __Jawabannya bisa!__

Sebelum saya membuat algoritmanya, mari saya perkenalkan dulu 2 cara tersebut.

### Merangkum dengan kata-kata sendiri

Dalam dunia _data science_, merangkum dengan kata-kata sendiri disebut dengan _abstraction based summarization_.

Contoh sederhananya adalah sebagai berikut. Misalkan saya punya kalimat:

> Manchester United baru saja kalah dari RB Leipzig dengan skor 3-2 di laga penyisihan Liga Champion Eropa/

Maka hasil _abstraction based summarization_-nya adalah:

> Manchester United dikalahkan RB Leipzig.

_Summary_ yang dihasilkan adalah kalimat baru hasil _rephrase_ dari konteks tulisan. Untuk bisa melakukan hal ini, kita perlu membuat model _deep learning_ yang rumit dan melatih model dengan berbagai macam prinsip-prinsip __NLP__ atau _language modelling_.

### Merangkum dengan menuliskan kata-kata yang sesuai di buku

Dalam dunia _data science_, merangkum dengan cara ini disebut dengan _extraction based summarization_.

Cara kerjanya relatif lebih mudah dibandingkan dengan cara pertama. Yakni dengan menentukan:

1. Kalimat yang `penting`.
1. Kalimat yang `sama`.

Saya akan membuat algoritma berdasarkan cara kedua ini.

---

# _Extraction Based Summarization_

## _Framework_

Misalkan saya memiliki suatu tulisan atau artikel atau buku yang panjang dan saya diharuskan untuk merangkumnya. Cara yang akan saya lakukan adalah menentukan kalimat mana saja yang penting. Setelah itu, dari kalimat-kalimat tersebut, saya akan cek bagaimana _similarity_ dari kata-kata tersebut.

Logikanya, jika terdapat dua atau lebih kalimat yang `sama`, saya hanya perlu menuliskan satu kalimat saja. Tidak perlu semuanya.

Berdasarkan _framework_ sederhana itu, saya akan membuat algoritmanya sebagai berikut:

```{r,echo=FALSE,fig.retina=10}
nomnoml("#direction: down,
        [Raw Data|Raw text] -> [Pre-Processing]
        [Pre-Processing] -> [Importance Screening]
        [Importance Screening] -> [Similarity Checking]
        [Similarity Checking] -> [Summarizing]
        
        [Pre-Processing|
          [Extract] -> [Paragraph]
          [Paragraph] -> [Sentences]
          [Sentences] -> [Pre-Processing]
          [Pre-Processing] -> [Lowercase]
          [Pre-Processing] -> [Remove]
          [Pre-Processing] -> [Stem]
          [Remove] -> [Stopwords]
          [Remove] -> [Punctuation|Kecuali titik]
          [Remove] -> [Numbers]
          [Stem] -> [Into root words]
          ]
        
        [Importance Screening|
          [Word counting] -> [Important words]
          [Important words] -> [Scoring|Relative score]
          [Scoring] -> [Sentences scoring]
          [Sentences scoring] -> [Important sentences]
          ]
        
        [Similarity Checking|
          [Measuring sentences similarity] -> [Cosine similarity]
          [Cosine similarity] -> [Distance matrix|Sentences as nodes\nSimilarity as edges]
          [Distance matrix] -> [Network Analysis]
          [Network Analysis] -> [Centrality|High Degree Nodes]
          ]
          
        [Summarizing|
          [Which sentences] -> [are important?]
          [Which sentences] -> [are centroids?]
          ]
        ")
```


Dari kesederhanaan _framework_ di atas, ada beberapa titik kritis yang perlu diperhatikan, yakni:

- _Pre-processing_
  - Dari tulisan yang ada, kita perlu mengetahui kalimat mana masuk ke dalam paragraf mana. Untuk itu, tanda `titik` menjadi krusial sebagai pembeda kalimat. Namun pada kenyataannya, `titik` tidak hanya digunakan sebagai penanda antar kalimat tapi bisa digunakan sebagai pengganti `koma` dalam _metric_ atau _currency_. 
  - Oleh karena itu, perlu ada algoritma yang bisa melakukan _tricky parts_ semacam ini memanfaatkan _regex_ (_reguler expression_) dari pola-pola karakter yang ada.
- _Importance_
  - Ini adalah proses yang termudah dari sekian langkah yang ada. Prinsipnya adalah _word counting_. Secara logika, kata-kata yang sering muncul merupakan kata yang penting dalam tulisan tersebut.
  - Agar tidak terjadi bias, maka _stopwords_ dan _numbers_ harus sudah dihapus di tahap _pre-processing_.
  - Langkah yang penting di sini adalah mengembalikan _importance score_ per kata kembali ke kalimat awalnya. Lalu menghitung _importance score_ dari masing-masing kalimat. 
  - Agar tidak terjadi bias, _importance score_ suatu kalimat dihitung dengan cara: $\frac{\Sigma score_{words}}{n_{words}}$.
- _Similarity_
- _Summarizing_