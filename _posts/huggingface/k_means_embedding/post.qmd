---
title: "Text Clustering Menggunakan Word Embedding"
format: 
  gfm:
    html-math-method: webtex
    toc: false
    toc-depth: 4
    fig-dpi: 500
editor: visual
execute:
  echo: false
  warning: false
  error: false
---

Setahun yang lalu, saya sempat menuliskan bagaimana caranya [melakukan *clustering* terhadap data teks berupa komentar atau *review* terhadap *oat drink* merek Tropicana Slim](https://ikanx101.com/blog/clustering-oat/). Lima tahun yang lalu, saya menuliskan cara [melakukan pengelompokan data teks menggunakan metode LDA](https://ikanx101.com/blog/blog-posting-sunyi/).

*Jujurly*, saya belum puas terhadap hasil keduanya karena masih terlalu kuantitatif dan tidak melibatkan konteks dari teks tersebut.

> Belum cukup pintar untuk bisa membaca dan mengelompokan konteks dari teks yang ada.

Begitu pikir saya.

Dengan perkembangan *large language model* (**LLM**) yang sangat pesat beberapa bulan belakangan ini, saya mencari cara lain bagaimana melakukan teks *clustering* menggunakan *k-means* tapi dengan proses **pembacaan konteks yang lebih pintar**.

*Flowchart*-nya kira-kira sebagai berikut:

![](nomnoml.png){width="250"}

Titik kritis yang membedakan analisa kali ini dengan analisa *clustering* sebelumnya terletak pada saat melakukan *word embedding* yang dibantu **LLM**. Model yang saya gunakan adalah `firqaaa/indo-sentence-bert-base` yang kita bisa dapatkan di situs [**Huggingface**](https://huggingface.co/firqaaa/indo-sentence-bert-base).

Semua proses ini saya lakukan di **R** dengan *environment* `Python3` yang saya panggil menggunakan `library(reticulate)`. Oleh karena itu, pastikan _local computer_ sudah ter-_install_ __R__ dan `Python3` _ya_. Jika kalian hendak menggunakan _Google Colab_ juga dipersilakan.

Bagaimana caranya? *Cekidots!*

## Data yang Digunakan

Data yang saya gunakan adalah 20 baris teks berisi komplain *customer* **sintetis** yang saya buat menggunakan Gemini Google berikut:

```{r}
#| include: false

complaints = readLines("komplen.txt")
```

```{r}
#| echo: false

complaints
```

Kita akan cek apakah hasil _clustering_ saya saat ini lebih baik dibandingkan sebelumnya atau tidak.

## Langkah Pengerjaan

### Tahap I

Langkah awal pengerjaan adalah _setting_ awal _environment_ `Python3`. Rekan-rekan bisa _copy, paste, and run_ skrip berikut di __R__:

```
system("pip install sentence-transformers")
```

Kegunaan skrip tersebut adalah meng-_install_ sistem `sentence-transformers` ke dalam _local computer_.

Selanjutnya adalah memanggil semua _libraries_ yang diperlukan:

```
# Load necessary libraries
library(reticulate)
library(readr)
library(dplyr)
```

Kemudian membuat __R__ menggunakan _environment_ dari `Python3`. Silakan dimodifikasi _path_ sesuai kebutuhan.

```
use_python("/usr/bin/python3")
```

Berikutnya kita _load_ _library_ di `Python3` berikut:


```
transformers <- reticulate::import("sentence_transformers")
```

Kemudian kita unduh model _word embedding_ yang diperlukan:

```
model <- transformers$SentenceTransformer('firqaaa/indo-sentence-bert-base')
```

Proses ini mungkin memakan waktu yang relatif lama karena ukurannya yang cukup besar.

Kemudian saya akan memasukkan kembali komplen konsumen dari _file_ `komplen.txt` yang ada.

```
complaints = readLines("komplen.txt")
```

### Tahap II

Selanjutnya saya akan lakukan _word embedding_ dari model yang ada. Kemudian saya ubah menjadi matriks jarak untuk keperluan _k-means clustering_.

```
complaint_embeddings <- model$encode(complaints)
embeddings_matrix    <- as.matrix(reticulate::py_to_r(complaint_embeddings))
```

### Tahap III

Selanjutnya kita akan menentukan nilai _k_ untuk _k-means clustering_. Untuk melakukannya, saya akan menghitung nilai _total within_ per masing-masing _clusters_ jika diandaikan suatu nilai _k_ tertentu.


```{r}
#| include: false

library(dplyr)
library(ggplot2)

rm(list=ls())
gc()

load("bahan_blog.rda")
```

Saya buat _function_ khusus berikut dan akan hitung seandainya nilai $k = 1,2,..,7$ kemudian saya buat grafiknya:

```{r}
#| echo: true

# function perhitungan total within
within_diff = function(k_input){
  kmeans_result = kmeans(embeddings_matrix, centers = k_input,iter.max = 50)
  output        = kmeans_result$tot.withinss
  return(output)
}

# perhitungan functionnya
tot = sapply(1:7,within_diff)

# membuat grafiknya
data.frame(k = 1:7,
           tot) |>
  ggplot(aes(x = factor(k),y = tot)) +
  geom_line(group = 1) +
  theme_classic() +
  labs(x = "How many k",
       y = "Total within")
```

Grafik _elbow_ di atas menunjukkan bahwa $k=3$ bisa dipilih sebagai salah satu alternatif banyaknya _cluster_ yang terbentuk. 

Selain itu, saya coba buatkan grafik _principal component analysis_ untuk melihat berapa kira-kira _clusters_ yang pantas.


```{r}
#| echo: false

num_clusters  <- 4
kmeans_result <- kmeans(embeddings_matrix, centers = num_clusters,iter.max = 100)

complaints = 
  complaints |> 
  mutate(cluster = kmeans_result$cluster)


pca <- prcomp(embeddings_matrix, center = TRUE, scale. = TRUE)
pca_data <- as.data.frame(pca$x[,1:2])
#pca_data$cluster <- factor(complaints$cluster)

ggplot(pca_data, aes(x = PC1, y = PC2, 
                     #color = cluster
                     )) +
  geom_point() +
  theme_minimal() +
  labs(title = "Customer Complaints Clusters", x = "PC1", y = "PC2")
```

Jika kita lihat, dugaannya nilai $k=3$ masih _acceptable_ namun saya ingin mencoba nilai $k=4$. Berikut adalah PCA dan hasilnya:

```{r}
pca <- prcomp(embeddings_matrix, center = TRUE, scale. = TRUE)
pca_data <- as.data.frame(pca$x[,1:2])
pca_data$cluster <- factor(complaints$cluster)

ggplot(pca_data, aes(x = PC1, y = PC2, 
                     color = cluster
                     )) +
  geom_point() +
  theme_minimal() +
  labs(title = "Customer Complaints Clusters", x = "PC1", y = "PC2")

```

_Cluster_ final:

```{r}
#| echo: false

head(complaints %>% 
       group_by(cluster) %>% 
       summarise(complaints = paste(complaints, collapse = " "))
     ) |> 
  knitr::kable()
```


```{r}
library(factoextra)
library(fpc)
library(dbscan)


set.seed(240)  # Setting seed
dist.mat<-dist(embeddings_matrix)

Hierar_cl = hclust(dist.mat, method = "complete")

plot(Hierar_cl)

fit = cutree(Hierar_cl, k = 10)
table(fit)
plot(Hierar_cl)
rect.hclust(Hierar_cl, k = 10, border = "red")

```


```{r}
complaints = 
  complaints |> 
  mutate(cluster = fit)

```


```{r}
#| echo: false

head(complaints %>% 
       group_by(cluster) %>% 
       summarise(complaints = paste(complaints, collapse = " "))
     ) |> 
  knitr::kable()
```


```{r}
pca <- prcomp(embeddings_matrix, center = TRUE, scale. = TRUE)
pca_data <- as.data.frame(pca$x[,1:2])
pca_data$cluster <- factor(complaints$cluster)

ggplot(pca_data, aes(x = PC1, y = PC2, 
                     color = cluster
                     )) +
  geom_point() +
  theme_minimal() +
  labs(title = "Customer Complaints Clusters", x = "PC1", y = "PC2")

```


Terlihat bahwa _clusters_ yang terbentuk memiliki kesamaan konteks di internal _cluster_ dan perbedaan konteks antar _cluster_.

Untuk menamakan cluster ini, saya akan coba rangkum menggunakan bantuan __Gemini__ sebagai berikut:

1. __Cluster 1__:
1. __Cluster 2__:
1. __CLuster 3__:

## Apa Gunanya *Clustering* Ini?

Bagi saya yang setiap hari berkecimpung di dunia _market research_, pada setiap survey biasanya memiliki setidaknya satu pertanyaan _open ended_ yang harus dikelompokkan terlebih dahulu agar bisa dianalisa lebih lanjut.

`if you find this article, please support this blog by clicking the ads`.