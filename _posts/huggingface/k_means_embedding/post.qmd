---
title: "Pengelompokkan Data Teks Menggunakan LLM Word Embedding dan Hierarchical CLustering"
format: 
  gfm:
    html-math-method: webtex
    toc: false
    toc-depth: 4
    fig-dpi: 500
editor: visual
execute:
  echo: false
  warning: false
  error: false
---

Setahun yang lalu, saya sempat menuliskan bagaimana caranya [melakukan *clustering* terhadap data teks berupa komentar atau *review* terhadap *oat drink* merek Tropicana Slim](https://ikanx101.com/blog/clustering-oat/). Lima tahun yang lalu, saya menuliskan cara [melakukan pengelompokan data teks menggunakan metode LDA](https://ikanx101.com/blog/blog-posting-sunyi/).

*Jujurly*, saya belum puas terhadap hasil keduanya karena masih terlalu kuantitatif dan tidak melibatkan konteks dari teks tersebut.

> Belum cukup pintar untuk bisa membaca dan mengelompokan konteks dari teks yang ada.

Begitu pikir saya.

Dengan perkembangan *large language model* (**LLM**) yang sangat pesat beberapa bulan belakangan ini, saya mencari cara lain bagaimana melakukan pengelompokkan teks menggunakan metode _clustering_ seperti _k-means_ atau _hierarchical_ tapi dengan proses **pembacaan konteks yang lebih pintar**.

*Flowchart*-nya kira-kira sebagai berikut:

![](nomnoml.png){width="450"}

Titik kritis yang membedakan analisa kali ini dengan analisa *clustering* sebelumnya terletak pada saat melakukan *word embedding* yang dibantu **LLM**. Model yang saya gunakan adalah `firqaaa/indo-sentence-bert-base` yang kita bisa dapatkan di situs [**Huggingface**](https://huggingface.co/firqaaa/indo-sentence-bert-base).

Semua proses ini saya lakukan di **R** dengan *environment* `Python3` yang saya panggil menggunakan `library(reticulate)`. Oleh karena itu, pastikan _local computer_ sudah ter-_install_ __R__ dan `Python3` _ya_. Jika kalian hendak menggunakan _Google Colab_ juga dipersilakan.

Bagaimana caranya? *Cekidots!*

## Data yang Digunakan

Data yang saya gunakan adalah 20 baris teks berisi komplain *customer* **sintetis** yang saya buat menggunakan Gemini Google berikut:

```{r}
#| include: false

complaints = readLines("komplen.txt")
```

```{r}
#| echo: false

complaints
```

Kita akan cek apakah hasil _clustering_ saya saat ini lebih baik dibandingkan sebelumnya atau tidak.

## Langkah Pengerjaan

### Tahap I

Langkah awal pengerjaan adalah _setting_ awal _environment_ `Python3`. Rekan-rekan bisa _copy, paste, and run_ skrip berikut di __R__:

```
system("pip install sentence-transformers")
```

Kegunaan skrip tersebut adalah meng-_install_ sistem `sentence-transformers` ke dalam _local computer_.

Selanjutnya adalah memanggil semua _libraries_ yang diperlukan:

```
# Load necessary libraries
library(reticulate)
library(readr)
library(dplyr)
```

Kemudian membuat __R__ menggunakan _environment_ dari `Python3`. Silakan dimodifikasi _path_ sesuai kebutuhan.

```
use_python("/usr/bin/python3")
```

Berikutnya kita _load_ _library_ di `Python3` berikut:


```
transformers <- reticulate::import("sentence_transformers")
```

Kemudian kita unduh model _word embedding_ yang diperlukan:

```
model <- transformers$SentenceTransformer('firqaaa/indo-sentence-bert-base')
```

Proses ini mungkin memakan waktu yang relatif lama karena ukurannya yang cukup besar.

Kemudian saya akan memasukkan kembali komplen konsumen dari _file_ `komplen.txt` yang ada.

```
complaints = readLines("komplen.txt")
```

### Tahap II

Selanjutnya saya akan lakukan _word embedding_ dari model yang ada. Kemudian saya ubah menjadi matriks jarak untuk keperluan _k-means clustering_.

```
complaint_embeddings <- model$encode(complaints)
embeddings_matrix    <- as.matrix(reticulate::py_to_r(complaint_embeddings))
```

### Tahap III

Tahap selanjutnya adalah melakukan _clustering_ dari data `embedding_matrix` hasil keluaran dari __LLM__. _Nah_ sebelum menentukan apakah saya akan menggunakan metode _k-means_ atau _hierarchical_, saya akan coba membuat visualisasi dari matriks tersebut.

Sebagai informasi, matriks tersebut berukuran $20 \times 768$ alias berdimensi tinggi. Agar memudahkan membuat visualisasinya, saya akan lakukan _dimension reduction_ dengan _principal component analysis_ (__PCA__). Berikut adalah grafik __PCA__-nya


```{r}
#| include: false

library(dplyr)
library(ggplot2)

rm(list=ls())
gc()

load("bahan_blog.rda")
```

```{r}
#| echo: false

pca <- prcomp(embeddings_matrix, center = TRUE, scale. = TRUE)
pca_data <- as.data.frame(pca$x[,1:2])
#pca_data$cluster <- factor(complaints$cluster)

ggplot(pca_data, aes(x = PC1, y = PC2, 
                     #color = cluster
                     )) +
  geom_point() +
  theme_minimal() +
  labs(title = "PCA dari 20 Komplain", x = "PC1", y = "PC2")
```

Jika saya lihat grafik di atas, saya bisa menggunakan metode:

1. _k-means_ untuk $k=3$ atau $k=4$, ATAU
1. _Hierarchical cluster_ dengan metode perhitungan jarak _complete_ (_max_) dengan _cut off cluster_ di `3` atau `4`.

Kedua metode tersebut akan menghasilkan hasil yang sama (secara teori). Sebagai penjelasan _how to do clustering_, rekan-rekan bisa melihat [tulisan saya sebelumnya ini](https://ikanx101.com/blog/clustering-R/).

### Tahap IV

Selanjutnya saya akan lakukan _hierarchical clustering_ dengan membuat _dendogram_ dengan _cut off_ sebanyak `3` _clusters_ berikut ini:

```{r}
library(factoextra)
library(fpc)

set.seed(240)  # Setting seed
dist.mat  <- dist(embeddings_matrix,method = "euclidean")
Hierar_cl <- hclust(dist.mat, method = "complete")

fit = cutree(Hierar_cl, k = 3)
plot(Hierar_cl)
rect.hclust(Hierar_cl, k = 3, border = "red")

```

Sekarang kita lihat bentuk akhir grafik __PCA__ dengan _clusters_ yang sudah final:

```{r}
#| include: false
complaints = complaints |> mutate(cluster = fit)
```


```{r}
#| echo: false

pca <- prcomp(embeddings_matrix, center = TRUE, scale. = TRUE)
pca_data <- as.data.frame(pca$x[,1:2])
pca_data$cluster <- factor(complaints$cluster)

ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster
                     )) +
  geom_point() +
  theme_minimal() +
  labs(title = "PCA dari 20 Komplain", x = "PC1", y = "PC2")
```

Berikut adalah hasil pengelompokkan komplain konsumen final:

```{r}
#| echo: false

head(complaints %>% 
       group_by(cluster) %>% 
       summarise(complaints = paste(complaints, collapse = " "))
     ) |> 
  knitr::kable()
```


Jika kita baca secara seksama, terlihat bahwa _clusters_ yang terbentuk sudah memiliki kesamaan konteks di internal _cluster_ dan perbedaan konteks antar _cluster_. Namun tetap kita bisa menemukan ada tiga pernyataan di _cluster 1_ yang tampaknya __salah masuk cluster__, yakni:

1. __Sinyal XL sering hilang saat bepergian menggunakan transportasi umum__.
1. __Kecepatan internet XL masih kalah cepat dibandingkan dengan provider lain di kota besar__.
1. __Aplikasi XL Center seringkali mengalami error saat digunakan__.


Untuk __pernyataan 1__ seharusnya lebih cocok masuk ke dalam __cluster 2__ sedangkan __pernyataan 2__ lebih cocok masuk ke dalam __cluster 3__. Sementara __pernyataan 3__ memang seharusnya menjadi _cluster_ baru.

> Ternyata penggunaan _word embedding_ dan pemilihan metode perhitungan _hierarchical cluster_ menjadi hal yang kritis. 

Saya sudah mencoba beberapa kemungkinan metode perhitungan _hierarchical cluster_ dan memang saat memilah _dendogram_ menjadi cluster yang lebih banyak saya bisa dapatkan hasil yang lebih baik. _Nah_, ini untuk keperluan contoh, __saya rasa tiga cluster sudah cukup__.

### Tahap V

Untuk memberikan nama atau istilah umum dari masing-masing _cluster_, saya akan coba rangkum menggunakan bantuan __Gemini__ sebagai berikut:

1. __Cluster 1__: XL memberikan layanan internet yang cukup baik, terutama dalam hal kecepatan dan fitur tambahan. Namun, kualitas jaringan masih perlu ditingkatkan di beberapa area.
1. __Cluster 2__: Sinyal XL sering lemah dan tidak stabil di berbagai kondisi.
1. __Cluster 3__: Layanan XL perlu ditingkatkan di berbagai aspek, seperti kuota, kecepatan, dan harga.

## Apa Gunanya *Clustering* Ini?

Bagi saya yang setiap hari berkecimpung di dunia _market research_, metode pengelompokan data teks yang saya lakukan saat ini sangat berguna. Pada setiap survey, biasanya ada setidaknya satu pertanyaan _open ended_ yang harus dikelompokkan terlebih dahulu agar bisa dianalisa lebih lanjut.

Metode ini memberikan satu cara alternatif yang lebih cepat untuk melakukan analisa teks yang banyak dalam waktu yang lebih singkat.

`if you find this article, please support this blog by clicking the ads`.