---
title: "Text Analysis: Review Konsumen Pemanis Rendah Kalori"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/cloud/project/_posts/home tester club/tulisan kedua")
library(dplyr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(ggraph)
library(igraph)
load("bahan_blog (1).rda")
```

Melanjutkan [tulisan pertama](https://ikanx101.com/blog/review-htci/) saya terkait _hometesterclub Indonesia_, kali ini saya akan coba membuat analisa sederhana dari data komentar konsumen terhadap dua buah produk pemanis rendah kalori yang ada di pasaran.

Apa saja?

1. Tropicana Slim Stevia.
1. Diabetasol Sweetener.

```{r,echo=FALSE}
library(EBImage)
gbr1 = readImage("p1.jpg")
gbr2 = readImage("p2.jpg")

par(mfrow = c(2,1))
plot(gbr2)
plot(gbr1)
```

Data komentar saya _scrape_ pagi ini (`3 Desember 2020 9.46 WIB`).

---

## Bagaimana cara _scrape_-nya?

Seperti biasa, saya menggunakan __R__ untuk membuat algoritma _web scraping_ dengan memanfaatkan `library(rvest)`.

```{r}
scrape_komen = function(url){
  read_html(url) %>% 
  html_nodes(".review-user_comment") %>% 
  html_text()
}
```

Cukup aplikasikan _function_ di atas ke halaman _review_ produk di situs _hometesterclub Indonesia_.

Data yang saya dapatkan adalah sebagai berikut:

```{r,echo = FALSE}
print("10 data pertama hasil scrape")
head(data,10)
```

Kali ini saya akan membandingkan komentar _member_ terhadap kedua produk yang _head to head_ di pasaran. Ada beberapa analisa yang hendak saya lakukan untuk membandingkannya, yakni:

1. _Wordcloud_,
1. _Bigrams_,
1. _Topic modelling_,
1. _Crosswords_,
1. _Log odds ratio_.

---

## _Pre-Processing_

Hal standar yang harus dilakukan setiap kali melakukan _text analysis_ adalah dengan membersihkan data terlebih dahulu dari tanda baca dan _stopwords_. Kali ini sengaja saya tidak melakukan _stemming_ karena saya masih belum mendapatkan algoritma terbaik untuk melakukan _stemming_ dalam Bahasa Indonesia. 

_Stopwords_ Bahasa Indonesia saya ambil dari [sini](https://raw.githubusercontent.com/stopwords-iso/stopwords-id/master/stopwords-id.txt).

Hasilnya sebagai berikut:

```{r,echo = FALSE}
stop = readLines("https://raw.githubusercontent.com/stopwords-iso/stopwords-id/master/stopwords-id.txt")
stop = c(stop,"saya","kamu","kamu","kita","aku")

pre_data %>%
  unnest_tokens("words",komen) %>%
  filter(!komen %in% stop)
  
print("10 data pertama hasil pre processing")
head(predata,10)
```

---

## _Wordcloud_
