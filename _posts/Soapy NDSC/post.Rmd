---
title: "Belajar Banyak Dari Shopee National Data Science Competition 2020"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/ikanx101.github.io/_posts/Soapy NDSC")
#setwd("~/Documents/ikanx101/_posts/Soapy NDSC")
library(dplyr)
library(EBImage)
library(ggplot2)
```

Sore ini adalah akhir dari rangkaian acara __Shopee National Data Science Competition 2020__. Suatu kompetisi yang sangat seru dan berbeda dengan [kompetisi _data science_ yang dulu](https://passingthroughresearcher.wordpress.com/2018/11/15/belajar-dari-finhacks-id-2018/) pernah saya ikuti. Walaupun _gak_ menang dan hanya mendapatkan peringkat yang _gak_ sebegitu bagusnya, saya ingin bercerita mengenai pengalaman seru ini.

---

# Pendaftaran

Di awal tahun 2020, ada salah seorang teman kuliah saya dulu di Matematika ITB 2004 mengirimkan pesan _WhatsApp_. Ia menginformasikan bahwa ada kompetisi _data science_ yang akan diselenggarakan oleh Shopee pada bulan Maret 2020. Kompetisi itu bertajuk _National Data Science Competition_ (NDSC). Ada `2` kelas yang dipertandingkan:

1. _Beginner_.
1. _Advance_.

Kompetisi itu bisa diikuti secara perorangan dan tim. Sebagaimana pengalaman saya di kompetisi sebelumnya, alangkah lebih baik jika mengikutinya secara tim. Oleh karena itu, kami coba mencari lagi siapa yang bisa diajak kolaborasi bareng. Dapatlah `2` orang tambahan:

1. Satu orang merupakan teman seangkatan kami dulu.
1. Satu orang lainnya merupakan rekan kerja teman saya yang menginformasikan adanya kompetisi.

Setelah membentuk tim, kami mendaftar. Awalnya, kami kira bisa mendaftar di kelas _beginner_, ternyata pada saat pendaftaran, tim panitia dari Shopee benar-benar menyeleksi peserta dengan ketat. Singkat cerita, kami (terpaksa) mendaftar di kelas _advance_.

> Tenang saja, rekan kerja saya yang satu ini jago banget kok.

Ujar teman saya itu. _Hehe_

---

# Pandemi Covid

Tidak disangka pada bulan Maret 2020, negara api (baca: pandemi Covid 19) menyerang Indonesia. Perhelatan yang awalnya akan dilakukan dengan cara `karantina` peserta di suatu tempat selama seharian penuh ternyata diundur ke waktu yang tidak bisa ditentukan.

Sampai akhirnya pada awal bulan November 2020, _email_ surat cinta itupun datang.

Kompetisi akan tetap digelar secara _online_ menggunakan platform __Kaggle__ pada `28 November 2020`. Lalu semua tim diharapkan untuk bisa mendaftar ulang jika ada perubahan komposisi anggota.

Nah, pada waktu ini, rekan kerja teman saya itu tiba-tiba menghilang dari grup _WhatsApp_ yang kami bentuk. 

> _But the show must go on(line)!_

_Bismillah_, kami tetap maju dengan komposisi yang ada. _Toh_ niat kita awalnya bukan untuk menang tapi untuk mencari pengalaman saja.

---

# _Pre Competition_: 22 November 2020

Seminggu tepat sebelum hari kompetisi, ada _email_ surat cinta lagi. Setiap tim diwajibkan mengikuti _pre competition_ yang diadakan di __Kaggle__. Apa itu?

## _Problem Statement_


```{r out.width="60%",echo=FALSE}
plot = readImage("problem.png")
plot(plot)
```

_Shopee_ mengedepankan _fairness_ terhadap semua barang yang dijual oleh _seller_-nya. Bisa jadi sepasang barang memiliki _title_ berbeda dan poto yang sedikit berbeda TAPI pada kenyataannya merupakan sepasang barang yang sama.

> Mungkin selama ini kita hanya mengetahui _fraud_ berupa penipuan yang ada di _marketplace_. Bisa jadi _fraud_ dilakukan dengan cara yang halus semacam ini.

Jadi tujuan _pre competition_ kali ini adalah membuat model _machine learning_ yang bisa memprediksi apakah sepasang barang merupakan barang yang sama atau beda dengan bermodalkan informasi:

1. Sepasang _titles_ barang.
1. Sepasang _images_ barang.

Data yang digunakan murni hanya `6` variabel saja!

```{r,echo=FALSE}
print("Contoh dataset yang diberikan:")
data = read.csv("new_training_set.csv")
head(data,5)
```

Dataset di atas ada `10.181` baris. Sedangkan data _images_ melebihi 1 GB ukurannya.

### Bagaimana cara menyelesaikannya?

Melihat data di atas, kami murni menghadapi _unstructured data_. Jadi langkah awal yang harus kami lakukan adalah mengekstrak _features_ variabel _titles_ dan _images_. Tentunya variabel _titles_ harus dibersihkan dulu dari tanda baca yang tidak diperlukan.

Setelah berdiskusi, maka ada beberapa _features_ yang akan kami ekstrak:

- _Titles_
  - Berapa banyak kata dari dua _titles_ barang?
  - Berapa banyak karakter dari dua _titles_ barang?
  - Berapa banyak kata yang sama dari dua _titles_ barang?
  - Berapa banyak _n-grams_ dari dua _titles_ barang? Terkait _n-grams_ ini, kami sampai menghitung kesamaan __bigrams__ hingga __7-grams__.
  - Berapa banyak angka yang sama dari dua _titles_ barang?
  - Berapa banyak huruf yang sama dari dua _titles_ barang? Ini kami hitung dari huruf __a__ sampai __z__.
  - Berapa jarak relatif antara dua vektor _character_ dari dua _titles_ barang?
  - Berapa jarak relatif antara dua vektor _character_ dari dua _titles_ barang (setelah dihilangkan semua angka)?
- _Images_. Ternyata apa yang saya lakukan [beberapa waktu lalu](https://ikanx101.com/blog/comparison-improvement/) saat _ngoprek_ data _images_ sangat berguna sekali.
  - Menghitung korelasi dari dua matriks _images_.
  - Menghitung proporsi kesamaan dua matriks _images_ untuk berbagai nilai sensitivitas.
  
Jadi kami fokus untuk mengekstrak semua informasi yang bisa diekstrak dari hari Minggu sampai Kamis (26 November 2020).
  
Singkat cerita, dari 4 variabel penting (2 _titles_ dan 2 _images_), kami berhasil mendapatkan `48` _features_. 

> _Features_ itu apa sih? Secara simpel, _features_ bisa saya katakan sebagai variabel numerik.

Oh iya, untuk mengekstrak _features_ dari `10.181` pasang _images_, saya membutuhkan waktu `1` jam penuh.

### Membuat model _machine learning_

Ada hal yang menarik saat kami mulai mengerjakan _pre competition_ ini. Pihak _Shopee_ merekomendasikan para peserta untuk menggunakan __Phyton__ daripada bahasa _data science_ lainnya.

Kami sebagai _R User_ garis keras merasa tertantang untuk tetap mengerjakannya dengan __R__. Ada satu dugaan dari saya:

> Jangan-jangan solusinya menggunakan deep learning.

Kenapa? Karena [_TensorFlow_](https://ikanx101.com/blog/deep-nutrisari/) yang digunakan untuk _Artificial Neural Network_ dibangun berbasis _Phyton_.

Harusnya tidak masalah, karena _TensorFlow_ bisa digunakan di __R__ juga.

Saya akhirnya mencoba membuat model _deep learning_, sementara teman saya yang lain _ngoprek_ _Support Vector Machine_ dan _XGBLinear_.

Hasilnya bagaimana?

Kami masih belum puas dengan _score_ yang dihasilkan saat dilakukan validasi.

### _Submission_ Jawaban Pertama: 26 November 2020

Di Kamis pagi, saya coba iseng membuat model _machine learning_ menggunakan _Random Forest Algorithm_. Tanpa disangka, _validation score_ yang saya dapatkan lebih tinggi dibandingkan model _deep learning_ yang saya buat sebelumnya.

Saat saya coba masukkan `data test pre competition` ke dalam model, lalu saya _submit_ ke _Kaggle_, saya terkejut karena mendapatkan nilai yang cukup tinggi: `0.89583` (max nilai = 1).

```{r out.width="60%",echo=FALSE}
plot = readImage("percobaan 1.jpg")
plot(plot)
```

Walaupun tidak termasuk ke dalam `10` besar, tapi _score_ `0.89583` masuk ke dalam __TOP 5__ _score_ teratas.

Kok bisa?

> Jadi ada beberapa tim yang memiliki _score_ yang sama.

```{r,echo=FALSE,fig.retina=10}
data = read.csv("precomp.csv")
data = janitor::clean_names(data)

data %>% 
  ggplot(aes(x = score)) +
  geom_density(fill = "darkred",
               alpha = .4) +
  geom_vline(xintercept = 0.89583,
             color = "steelblue") +
  theme_minimal() +
  annotate("label",
           x = .8,
           y = 3,
           label = "Posisi tim saya\n0.89583") +
  labs(title = "Sebaran Score Para Tim yang Berlaga di Pre Competition",
       subtitle = "Shopee National Data Science Competition 2020",
       labs = "Visualized using R\nikanx101.com",
       x = "Score",
       y = "Density") +
  theme(axis.text.y = element_blank())

```

Pada _pre competition_ ini, ada `r length(unique(data$team_name))` buah tim yang berkompetisi.

Sebagai orang yang biasa berkutat terkait otomasi di __R__, mendapatkan _prediction score_ segitu sudah membahagiakan bagi saya pribadi.

Analoginya seperti seorang tukang gado-gado yang memasak _beef wellington_ dan diberikan pujian _good effort_ oleh __Gordon Ramsay__. _hehe_.

---

# _Competition Day_: 28 November 2020

Akhirnya hari yang dinanti datang juga. Acara dimulai pada pukul 10.00 WIB dengan sambutan-sambutan dari pihak _Shopee_ dan Menristek (Prof. Bambang Brodjonegoro).

Seperti biasa, saat dibuka forum _QnA_ di _live chat_ _Zoom_, ada saja pertanyaan-pertanyaan kocak yang ditanyakan para peserta iseng. Berikut skrinsutnya:

```{r out.width="60%",echo=FALSE}
print("Boleh pakai excel?")
plot1 = readImage("excel.jpeg")
plot1 = rotate(plot1,-45) 
plot(plot1)

print("Boleh telepon rekan setim?")
plot1 = readImage("telepon.jpeg")
plot1 = rotate(plot1,-45) 
plot(plot1)

print("Apakah perlu kirim installernya?")
plot1 = readImage("installer.jpg")
plot(plot1)

print("Mimin apa kabar?")
plot1 = readImage("kabar.jpeg")
plot1 = rotate(plot1,-45) 
plot(plot1)
```

Pada sesi _QnA_ ini, ternyata _Shopee_ memperbolehkan penggunaan bahasa lain seperti __R__ dan _Matlab_.

Setiap tim hanya diperbolehkan melakukan `5` kali _submission_ jawaban.

### Pukul 11.00 WIB

Kompetisi kelas dimulai tepat pukul 11.00 WIB via _Kaggle_. Ternyata masalah yang dihadapi masih sama seperti masalah di _pre competition_.

Berikut adalah data yang digunakan pada kompetisi tadi siang:

```{r,echo=FALSE}
data = read.csv("new_test_set.csv")
head(data,10)
```

Ada `32.580` baris pasang data beserta 2 GB _images_ yang harus diselesaikan selama `3` jam.

Awalnya kami akan menggunakan model _random forest_ dan _XGBLinear_ kemarin yang sudah jadi dengan beberapa modifikasi penambahan beberapa _features_ pada variabel _character_.

Jadi ada dua hal yang kami kerjakan terkait variabel _titles_:

1. Menghapus _stopwords_ (Indonesia dan _English_).
1. Menghitung `tf_idf`.

Secara paralel, saya mengekstrak _features_ dari `32.580` pasang _images_.

### Pukul 12.46

Saya sudah selesai menyelesaikan masalah _stopwords_, sementara rekan saya yang lain telah menyelesaikan masalah `tf_idf`. 

Namun saya baru menyadari suatu hal penting:

> Kemarin, untuk mengekstrak 10 ribu pasang _images_ saya butuh waktu sejam.

Kini dengan banyaknya pasang _images_ yang tiga kali lipat, maka akan dibutuhkan waktu `3` jam. 

__Bisa jadi kami tidak akan menyelesaikan kompetisi ini tepat waktu!__

```{r  out.width="60%",echo=FALSE}
print("Chat insecure di grup WA")
plot = readImage("sadar.jpg")
plot(plot)
```

### Pukul 13.21 WIB

Ternyata benar, sampai tersisa 40 menit lagi saya masih belum selesai mengekstrak _features_ dari _images_ tersebut. Laptop saya hanya sempat menghitung matriks korelasi dari pasangan _images_.

> Cuma 1 _feature_ dari total 7 _features_ yang direncanakan.

Akhirnya kami membuat keputusan untuk menggunakan _features_ seadanya saja dan membuat model kembali dari _scratch_.

Karena saya sempat panik, akibatnya _training dataset_ yang jadi acuan antara saya dan rekan-rekan satu tim putus.

Akibatnya saya dan rekan akhirnya `berpisah` untuk membuat model sendiri-sendiri. Siapa yang tercepat dan percaya bahwa modelnya memberikan _validation score_ terbaik dipersilakan untuk _submit_ jawaban.

Saya coba buat kembali model _machine learning_ menggunakan _Random Forest Algorithm_. Pede dengan _validation score_-nya,saya coba _submit_ jawaban.

```{r  out.width="60%",echo=FALSE}
print("Chat insecure di grup WA")
plot1 = readImage("sadar 2.jpg")
plot2 = readImage("sadar 3.jpg")

par(mfrow=c(1,2))
plot(plot1);plot(plot2)
```

Hasilnya tim kami mendapatkan score `0.37905`.

Beberapa menit berselang, rekan setim saya yang lain mencoba submit jawaban dan hasilnya tetap sama-sama buruk `0.37106`.

> _Hopeless_...

Walaupun tidak ada niatan untuk menang TAPI tidak juga gugur dengan nilai  yang buruk seperti ini.

### Pukul 13.42 WIB

Setelah beristirahat sejenak, saya mulai berpikir.

> Kenapa _gak_ pakai _deep learning_ saja _yah_?  Barangkali bagus.

Dalam waktu seadanya, saya coba _build deep learning_ model di __TensorFlow__. Saking _ngasal_-nya, saya hanya buat `3` _layers_:

1. _Layer_ pertama berisi _nodes_ sebanyak tahun kelahiran saya.
1. _Layer_ kedua berisi _nodes_ sebanyak tanggal lahir saya.
1. _Layer_ ketiga berisi _nodes_ sebanyak tanggal lahir istri saya.

Lalu saya set _epoch_ (iterasi) sebanyak `20` kali saja. Lalu hasil prediksinya saya _submit_.

> Hasilnya ternyata menggembirakan!

_Score_ kami naik jadi `0.66994`.

```{r  out.width="60%",echo=FALSE}
print("Chat insecure di grup WA")
plot1 = readImage("NN akhirnya.jpg")
plot2 = readImage("NN akhirnya 2.jpg")

par(mfrow=c(1,2))
plot(plot1);plot(plot2)
```

### Pukul 13.57 WIB

Mendekati menit-menit akhir, setelah saya mengetahui bahwa _deep learning_ model memberikan _score_ yang lebih baik, saya coba _fine tuning_ kembali beberapa parameternya.

Sementara, rekan tim saya juga sudah berhasil membuat model lain dengan _XGBoost Algorithm_.

Dalam waktu yang relatif sama, kami sama-sama _submit_ jawaban final. 

Keduanya memiliki _score_ yang mirip-mirip, sekitar `0.73`. 

```{r  out.width="60%",echo=FALSE}
print("Chat insecure di grup WA")
plot1 = readImage("NN pertama.jpg")
plot2 = readImage("submit final.jpg")

par(mfrow=c(1,2))
plot(plot1);plot(plot2)
```

### Pukul 14.00 WIB ENDING

Akhirnya waktu menunjukkan pukul 14.00 WIB. Panitia mengumumkan bahwa _submission_ telah berakhir.

Kini,posisi tim saya adalah sebagai berikut:

```{r,echo=FALSE,fig.retina=10}
data = read.csv("final.csv")
data = janitor::clean_names(data)

data %>% 
  ggplot(aes(x = score)) +
  geom_density(fill = "darkred",
               alpha = .4) +
  geom_vline(xintercept = 0.73422,
             color = "steelblue") +
  theme_minimal() +
  annotate("label",
           x = .7,
           y = 2.5,
           label = "Posisi tim saya\n0.73422") +
  labs(title = "Sebaran Score Para Tim yang Berlaga di Competition",
       subtitle = "Shopee National Data Science Competition 2020",
       labs = "Visualized using R\nikanx101.com",
       x = "Score",
       y = "Density") +
  theme(axis.text.y = element_blank())

```

Dari total `r length(unique(data$team_name))` buah tim yang berkompetisi di hari ini.

---

# Summary

## _Key take points_

1. Kami tidak pernah berpikir bahwa kompetisi kali ini tetap melibatkan data _images_ dengan jumlah yang sangat besar. Jika mengetahuinya, mungkin kami bisa memodifikasi _function_ pembacaan _images_ sehingga secara komputasi bisa lebih cepat.
1. Tanpa disengaja saya _build_ dan _fine tune_ _deep learning model_ secepat kilat dengan hasil yang tidak mengecewakan. Tentunya ini akan menjadi bahasan yang bisa diulik di kemudian hari.
1. Walaupun diadakan secara virtual (dimana banyak distraksi di sana-sini), tapi keseruan dan greget kompetisi ini _dapet banget_.

> It is such a great experience for us! 
